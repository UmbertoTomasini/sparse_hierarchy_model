{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d14996-ba21-4820-8456-59407e3adc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, num_ch=3):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # Calculate padding for input size 36\n",
    "        padding_36 = 1  # For kernel_size=3, stride=1, padding=1\n",
    "        self.conv1 = nn.Conv1d(num_ch, 64, kernel_size=3,\n",
    "                               stride=1, padding=padding_36, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool1d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return torch.squeeze(out)\n",
    "\n",
    "\n",
    "def ResNet18(num_ch=3, num_classes=10):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes, num_ch=num_ch)\n",
    "\n",
    "\n",
    "def ResNet34(num_ch=3, num_classes=10):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, num_ch=num_ch)\n",
    "\n",
    "\n",
    "def ResNet50(num_ch=3, num_classes=10):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, num_ch=num_ch)\n",
    "\n",
    "\n",
    "def ResNet101(num_ch=3, num_classes=10):\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, num_ch=num_ch)\n",
    "\n",
    "\n",
    "def ResNet152(num_ch=3, num_classes=10):\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3], num_classes=num_classes, num_ch=num_ch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a44d099e-90e5-4db1-8050-6ccdfa68a021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    return x * x.sigmoid()\n",
    "\n",
    "\n",
    "def drop_connect(x, drop_ratio):\n",
    "    keep_ratio = 1.0 - drop_ratio\n",
    "    mask = torch.empty([x.shape[0], 1, 1], dtype=x.dtype, device=x.device)\n",
    "    mask.bernoulli_(keep_ratio)\n",
    "    x.div_(keep_ratio)\n",
    "    x.mul_(mask)\n",
    "    return x\n",
    "\n",
    "\n",
    "class SE(nn.Module):\n",
    "    '''Squeeze-and-Excitation block with Swish.'''\n",
    "\n",
    "    def __init__(self, in_channels, se_channels):\n",
    "        super(SE, self).__init__()\n",
    "        self.se1 = nn.Conv1d(in_channels, se_channels, kernel_size=1, bias=True)\n",
    "        self.se2 = nn.Conv1d(se_channels, in_channels, kernel_size=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.adaptive_avg_pool1d(x, 1)\n",
    "        out = swish(self.se1(out))\n",
    "        out = self.se2(out).sigmoid()\n",
    "        out = x * out\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    '''expansion + depthwise + pointwise + squeeze-excitation'''\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride,\n",
    "                 expand_ratio=1,\n",
    "                 se_ratio=0.,\n",
    "                 drop_rate=0.):\n",
    "        super(Block, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.drop_rate = drop_rate\n",
    "        self.expand_ratio = expand_ratio\n",
    "\n",
    "        # Expansion\n",
    "        channels = expand_ratio * in_channels\n",
    "        self.conv1 = nn.Conv1d(in_channels,\n",
    "                               channels,\n",
    "                               kernel_size=1,\n",
    "                               stride=1,\n",
    "                               padding=0,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(channels)\n",
    "\n",
    "        # Depthwise conv\n",
    "        self.conv2 = nn.Conv1d(channels,\n",
    "                               channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=stride,\n",
    "                               padding=(1 if kernel_size == 3 else 2),\n",
    "                               groups=channels,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(channels)\n",
    "\n",
    "        # SE layers\n",
    "        se_channels = int(in_channels * se_ratio)\n",
    "        self.se = SE(channels, se_channels)\n",
    "\n",
    "        # Output\n",
    "        self.conv3 = nn.Conv1d(channels,\n",
    "                               out_channels,\n",
    "                               kernel_size=1,\n",
    "                               stride=1,\n",
    "                               padding=0,\n",
    "                               bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        # Skip connection if in and out shapes are the same (MV-V2 style)\n",
    "        self.has_skip = (stride == 1) and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x if self.expand_ratio == 1 else swish(self.bn1(self.conv1(x)))\n",
    "        out = swish(self.bn2(self.conv2(out)))\n",
    "        out = self.se(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        if self.has_skip:\n",
    "            if self.training and self.drop_rate > 0:\n",
    "                out = drop_connect(out, self.drop_rate)\n",
    "            out = out + x\n",
    "        return out\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, cfg, num_ch=3, num_classes=10):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # Modify the first convolutional layer for input size [batch_size, num_channels, 36]\n",
    "        self.conv1 = nn.Conv1d(num_ch,\n",
    "                               32,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.layers = self._make_layers(in_channels=32)\n",
    "        self.linear = nn.Linear(cfg['out_channels'][-1], num_classes)\n",
    "\n",
    "    def _make_layers(self, in_channels):\n",
    "        layers = []\n",
    "        cfg = [self.cfg[k] for k in ['expansion', 'out_channels', 'num_blocks', 'kernel_size',\n",
    "                                     'stride']]\n",
    "        b = 0\n",
    "        blocks = sum(self.cfg['num_blocks'])\n",
    "        for expansion, out_channels, num_blocks, kernel_size, stride in zip(*cfg):\n",
    "            strides = [stride] + [1] * (num_blocks - 1)\n",
    "            for stride in strides:\n",
    "                drop_rate = self.cfg['drop_connect_rate'] * b / blocks\n",
    "                layers.append(\n",
    "                    Block(in_channels,\n",
    "                          out_channels,\n",
    "                          kernel_size,\n",
    "                          stride,\n",
    "                          expansion,\n",
    "                          se_ratio=0.25,\n",
    "                          drop_rate=drop_rate))\n",
    "                in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = swish(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = F.adaptive_avg_pool1d(out, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        dropout_rate = self.cfg['dropout_rate']\n",
    "        if self.training and dropout_rate > 0:\n",
    "            out = F.dropout(out, p=dropout_rate)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def EfficientNetB0(num_ch=3, num_classes=10):\n",
    "    cfg = {\n",
    "        'num_blocks': [1, 2, 2, 3, 3, 4, 1],\n",
    "        'expansion': [1, 6, 6, 6, 6, 6, 6],\n",
    "        'out_channels': [16, 24, 40, 80, 112, 192, 320],\n",
    "        'kernel_size': [3, 3, 5, 3, 5, 5, 3],\n",
    "        'stride': [1, 2, 2, 2, 1, 2, 1],\n",
    "        'dropout_rate': 0.2,\n",
    "        'drop_connect_rate': 0.2,\n",
    "    }\n",
    "    return EfficientNet(cfg, num_classes=num_classes, num_ch=num_ch)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# net = EfficientNetB0(num_ch=1, num_classes=10)\n",
    "# x = torch.randn(2, 1, 36)\n",
    "# y = net(x)\n",
    "# print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfbec91b-2020-40e2-9a1c-35cad1a51b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\"Setting attributes on ParameterList is not supported.\"\n",
    ")\n",
    "\n",
    "cfg = {\n",
    "    \"VGG11\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"VGG13\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
    "    \"VGG16\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n",
    "    \"VGG19\": [ 64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, 256, \"M\", 512, 512, 512, 512, \"M\", 512, 512, 512, 512, \"M\",],\n",
    "}\n",
    "\n",
    "class VGG1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vgg_name,\n",
    "        num_ch=3,\n",
    "        num_classes=10,\n",
    "        bias=True,\n",
    "        batch_norm=True,\n",
    "        pooling=\"max\",\n",
    "        pooling_size=2,\n",
    "        param_list=False,\n",
    "        width_factor=1,\n",
    "        stride=2,\n",
    "    ):\n",
    "        super(VGG1D, self).__init__()\n",
    "        if pooling == True:\n",
    "            pooling = \"max\"\n",
    "        self.features = self._make_layers(\n",
    "            cfg[vgg_name],\n",
    "            ch=num_ch,\n",
    "            bias=bias,\n",
    "            bn=batch_norm,\n",
    "            pooling=pooling,\n",
    "            ps=pooling_size,\n",
    "            param_list=param_list,\n",
    "            width_factor=width_factor,\n",
    "            stride=stride\n",
    "        )\n",
    "        stride_factor = 729 if stride == 1 else 1\n",
    "        self.classifier = nn.Linear(int(512 * width_factor) * stride_factor, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg, ch, bias, bn, pooling, ps, param_list, width_factor, stride):\n",
    "        layers = []\n",
    "        in_channels = ch\n",
    "        # Calculate padding for input size 36\n",
    "        padding_36 = max(0, ((36 - 1) * stride - 32 + ps - 1) // 2)\n",
    "        if ch == 1:\n",
    "            layers.append(nn.ZeroPad1d(padding_36))\n",
    "        if param_list:\n",
    "            convLayer = Conv1dList\n",
    "        else:\n",
    "            convLayer = nn.Conv1d\n",
    "        for x in cfg:\n",
    "            if x == \"M\":\n",
    "                if pooling == \"max\":\n",
    "                    layers += [\n",
    "                        nn.MaxPool1d(\n",
    "                            kernel_size=ps, stride=stride, padding=ps // 2 + ps % 2 - 1\n",
    "                        )\n",
    "                    ]\n",
    "                elif pooling == \"avg\":\n",
    "                    layers += [\n",
    "                        nn.AvgPool1d(\n",
    "                            kernel_size=ps, stride=stride, padding=ps // 2 + ps % 2 - 1\n",
    "                        )\n",
    "                    ]\n",
    "                else:\n",
    "                    layers += [SubSampling1D(kernel_size=ps, stride=stride)]\n",
    "            else:\n",
    "                x = int(x * width_factor)\n",
    "                if bn:\n",
    "                    layers += [\n",
    "                        convLayer(in_channels, x, kernel_size=3, padding=1, bias=bias),\n",
    "                        nn.BatchNorm1d(x),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                    ]\n",
    "                else:\n",
    "                    layers += [\n",
    "                        convLayer(in_channels, x, kernel_size=3, padding=1),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                    ]\n",
    "                in_channels = x\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class Conv1dList(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        bias=True,\n",
    "        padding_mode=\"constant\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        weight = torch.empty(out_channels, in_channels, kernel_size)\n",
    "\n",
    "        nn.init.kaiming_uniform_(weight, a=math.sqrt(5))\n",
    "\n",
    "        if bias is not None:\n",
    "            bias = nn.Parameter(\n",
    "                torch.empty(\n",
    "                    out_channels,\n",
    "                )\n",
    "            )\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(bias, -bound, bound)\n",
    "\n",
    "        n = max(1, 128 * 256 // (in_channels * kernel_size)) # * kernel_size\n",
    "        weight = nn.ParameterList(\n",
    "            [nn.Parameter(weight[j : j + n]) for j in range(0, len(weight), n)]\n",
    "        )\n",
    "\n",
    "        setattr(self, \"weight\", weight)\n",
    "        setattr(self, \"bias\", bias)\n",
    "\n",
    "        self.padding = padding\n",
    "        self.padding_mode = padding_mode\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        weight = self.weight\n",
    "        if isinstance(weight, nn.ParameterList):\n",
    "            weight = torch.cat(list(self.weight))\n",
    "\n",
    "        return F.conv1d(x, weight, self.bias, self.stride, self.padding)\n",
    "\n",
    "\n",
    "class SubSampling1D(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=None):\n",
    "        super(SubSampling1D, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride if (stride is not None) else kernel_size\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return input.unfold(2, self.kernel_size, self.stride)[..., 0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "471a0184-7602-4ca6-b01c-7528e384e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "net = ResNet18()#VGG1D('VGG11')#EfficientNetB0()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c037c85-571a-4eda-a89e-498cc5bf5b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, _ = get_graph_node_names(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eae81f30-8886-4e59-a327-72dfc7393aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x',\n",
       " 'conv1',\n",
       " 'bn1',\n",
       " 'relu',\n",
       " 'layer1.0.conv1',\n",
       " 'layer1.0.bn1',\n",
       " 'layer1.0.relu',\n",
       " 'layer1.0.conv2',\n",
       " 'layer1.0.bn2',\n",
       " 'layer1.0.add',\n",
       " 'layer1.0.relu_1',\n",
       " 'layer1.1.conv1',\n",
       " 'layer1.1.bn1',\n",
       " 'layer1.1.relu',\n",
       " 'layer1.1.conv2',\n",
       " 'layer1.1.bn2',\n",
       " 'layer1.1.add',\n",
       " 'layer1.1.relu_1',\n",
       " 'layer2.0.conv1',\n",
       " 'layer2.0.bn1',\n",
       " 'layer2.0.relu',\n",
       " 'layer2.0.conv2',\n",
       " 'layer2.0.bn2',\n",
       " 'layer2.0.shortcut.0',\n",
       " 'layer2.0.shortcut.1',\n",
       " 'layer2.0.add',\n",
       " 'layer2.0.relu_1',\n",
       " 'layer2.1.conv1',\n",
       " 'layer2.1.bn1',\n",
       " 'layer2.1.relu',\n",
       " 'layer2.1.conv2',\n",
       " 'layer2.1.bn2',\n",
       " 'layer2.1.add',\n",
       " 'layer2.1.relu_1',\n",
       " 'layer3.0.conv1',\n",
       " 'layer3.0.bn1',\n",
       " 'layer3.0.relu',\n",
       " 'layer3.0.conv2',\n",
       " 'layer3.0.bn2',\n",
       " 'layer3.0.shortcut.0',\n",
       " 'layer3.0.shortcut.1',\n",
       " 'layer3.0.add',\n",
       " 'layer3.0.relu_1',\n",
       " 'layer3.1.conv1',\n",
       " 'layer3.1.bn1',\n",
       " 'layer3.1.relu',\n",
       " 'layer3.1.conv2',\n",
       " 'layer3.1.bn2',\n",
       " 'layer3.1.add',\n",
       " 'layer3.1.relu_1',\n",
       " 'layer4.0.conv1',\n",
       " 'layer4.0.bn1',\n",
       " 'layer4.0.relu',\n",
       " 'layer4.0.conv2',\n",
       " 'layer4.0.bn2',\n",
       " 'layer4.0.shortcut.0',\n",
       " 'layer4.0.shortcut.1',\n",
       " 'layer4.0.add',\n",
       " 'layer4.0.relu_1',\n",
       " 'layer4.1.conv1',\n",
       " 'layer4.1.bn1',\n",
       " 'layer4.1.relu',\n",
       " 'layer4.1.conv2',\n",
       " 'layer4.1.bn2',\n",
       " 'layer4.1.add',\n",
       " 'layer4.1.relu_1',\n",
       " 'avg_pool1d',\n",
       " 'size',\n",
       " 'view',\n",
       " 'linear',\n",
       " 'squeeze']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36878b-e9eb-4f28-add1-7e394ffd947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_graph_node_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
